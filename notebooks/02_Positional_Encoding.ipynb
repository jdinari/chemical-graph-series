{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b6345d9",
   "metadata": {},
   "source": [
    "# üß™ Lesson 02: Positional Encoding\n",
    "\n",
    "**Series**: Chemical Graph Machine Learning  \n",
    "**Prerequisites**: Lesson 01 (molecular graphs, node/edge features)  \n",
    "**Next Lesson**: [03 - Graph Attention Networks](./03_GAT_Model.ipynb)  \n",
    "**Estimated Time**: 60-75 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d4acb",
   "metadata": {},
   "source": [
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. ‚úÖ Understand why standard GNNs struggle with positional information\n",
    "2. ‚úÖ Compute Laplacian matrices from molecular graphs\n",
    "3. ‚úÖ Extract spectral features using eigendecomposition\n",
    "4. ‚úÖ Implement Random Walk Positional Encoding (RWPE)\n",
    "5. ‚úÖ Compare different positional encoding strategies\n",
    "6. ‚úÖ Visualise how positional encodings capture molecular structure\n",
    "\n",
    "**Why this matters**: Without positional encoding, GNNs can't distinguish between molecules with the same local connectivity but different global structure (e.g., isomers).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b49fb",
   "metadata": {},
   "source": [
    "## üîÑ Quick Recap: What We Know\n",
    "\n",
    "From Lesson 01, you learned to:\n",
    "- Convert SMILES ‚Üí RDKit molecules ‚Üí NetworkX graphs\n",
    "- Extract node features: atomic number, charge, aromaticity, etc.\n",
    "- Build edge features: bond types and connectivity\n",
    "\n",
    "**Today's challenge**: Standard node features are *local* (they describe individual atoms). We need *global* positional information so the model knows where each atom sits in the overall molecular structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b26210",
   "metadata": {},
   "source": [
    "## üìñ Main Content Structure\n",
    "\n",
    "### Part 1: The Positional Encoding Problem\n",
    "- Why identical local neighbourhoods ‚â† same position\n",
    "- Graph isomorphism and the WL test\n",
    "- Chemical example: ortho vs meta vs para substitution\n",
    "\n",
    "### Part 2: Laplacian Eigenvectors\n",
    "- Building the graph Laplacian matrix\n",
    "- Eigendecomposition and spectral graph theory\n",
    "- Connection to molecular vibrations (bonus chemistry insight!)\n",
    "\n",
    "**Code**: Compute Laplacian eigenvectors for our molecule from Lesson 01\n",
    "\n",
    "### Part 3: Random Walk Positional Encoding (RWPE)\n",
    "- Simulating random walks on molecular graphs\n",
    "- Building the landing probability matrix\n",
    "- Why RWPE captures both local and global structure\n",
    "\n",
    "**Code**: Implement RWPE and compare to Laplacian PE\n",
    "\n",
    "### Part 4: Visualising Positional Encodings\n",
    "- Plotting eigenvector components\n",
    "- Understanding what different eigenvectors \"see\"\n",
    "- Sanity checks: symmetric molecules should have symmetric encodings\n",
    "\n",
    "**Code**: Create visualisations overlaying PE on molecular structure\n",
    "\n",
    "### Part 5: Integration with Feature Matrices\n",
    "- Concatenating positional encodings to node features\n",
    "- Choosing the number of eigenvectors (k parameter)\n",
    "- Normalisation and preprocessing considerations\n",
    "\n",
    "**Code**: Build final feature matrix: [atomic features | positional encoding]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db2dd98",
   "metadata": {},
   "source": [
    "## üí° Key Chemical Insights\n",
    "\n",
    "### Why chemists should care about positional encoding:\n",
    "- **Regioisomers**: ortho-xylene vs meta-xylene have identical local connectivity but different properties\n",
    "- **Ring systems**: PE distinguishes bridgehead atoms from others\n",
    "- **Symmetry**: PE respects molecular symmetry (automorphisms)\n",
    "- **Pharmacophores**: Spatial arrangement matters for binding ‚Üí PE captures this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69af379a",
   "metadata": {},
   "source": [
    "## ‚úÖ Knowledge Checkpoint\n",
    "\n",
    "Before moving to Lesson 03, ensure you can:\n",
    "\n",
    "- [ ] Explain why `[C, N]` bonds tell you nothing about molecular structure\n",
    "- [ ] Compute the Laplacian matrix for a simple graph\n",
    "- [ ] Interpret what the first few eigenvectors represent\n",
    "- [ ] Implement RWPE with different walk lengths\n",
    "- [ ] Decide how many positional dimensions to use\n",
    "\n",
    "**Self-test**: Take two isomers (e.g., n-butane vs isobutane: `CCCC` vs `CC(C)C`):\n",
    "1. Extract positional encodings for both\n",
    "2. Verify that central carbons have different PE despite same local features\n",
    "3. Visualise the difference\n",
    "\n",
    "If the PE distinguishes them, you've succeeded!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cfebf0",
   "metadata": {},
   "source": [
    "## üîÆ Coming Up in Lesson 03: Graph Attention Networks\n",
    "\n",
    "Now that we have rich node features (atomic properties + positional encoding), we can build our first proper GNN.\n",
    "\n",
    "**What you'll learn**:\n",
    "- Message passing: how information flows between neighbouring atoms\n",
    "- Attention mechanisms: learning which bonds are most important\n",
    "- Multi-head attention: capturing different types of chemical relationships simultaneously\n",
    "\n",
    "**What you'll need from today**:\n",
    "- The positional encoding functions we just built\n",
    "- Understanding that nodes need both local and global features\n",
    "- The feature matrix format (will feed directly into PyTorch Geometric)\n",
    "\n",
    "**The payoff**: By Lesson 07, these attention weights will show you *which atoms and bonds the model focuses on* when predicting solubility‚Äîinterpretable chemistry!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b9f5f",
   "metadata": {},
   "source": [
    "## üìñ Further Reading\n",
    "\n",
    "**Spectral Graph Theory**:\n",
    "- Chung, F. R. K. (1997). *Spectral Graph Theory*. AMS. [Classic textbook]\n",
    "- Von Luxburg, U. (2007). \"A tutorial on spectral clustering.\" *Statistics and Computing*.\n",
    "\n",
    "**Positional Encoding in GNNs**:\n",
    "- Dwivedi et al. (2021). \"Benchmarking Graph Neural Networks.\" *arXiv:2003.00982*\n",
    "- Satorras et al. (2021). \"E(n) Equivariant Graph Neural Networks.\" *ICML 2021*\n",
    "\n",
    "**Chemistry Connection**:\n",
    "- Normal modes of vibration use the same eigenvector math!\n",
    "- See any computational chemistry textbook on molecular vibrations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c34295",
   "metadata": {},
   "source": [
    "**Navigation**: [‚Üê Lesson 01](./01_Building_Graphs.ipynb) | [Lesson 03 ‚Üí](./03_GAT_Model.ipynb) | [Series Home](../README.md)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
